var documenterSearchIndex = {"docs":
[{"location":"models/nomad/#Nonlinear-Manifold-Decoders-for-Operator-Learning-(NOMADs)","page":"NOMAD","title":"Nonlinear Manifold Decoders for Operator Learning (NOMADs)","text":"","category":"section"},{"location":"models/nomad/","page":"NOMAD","title":"NOMAD","text":"NOMADs are similar to DeepONets in the aspect that they can learn when the input and output function spaces are defined on different domains. Their architecture is different and use nonlinearity to the latent codes to obtain the operator approximation. The architecture involves an approximator to encode the input function space, which is directly concatenated with the input function coordinates, and passed into a decoder net to give the output function at the given coordinate.","category":"page"},{"location":"models/nomad/","page":"NOMAD","title":"NOMAD","text":"beginalign*\nu(y) xrightarrowmathcalA   beta \n quad searrow\n quad quad mathcalG_theta u(y) = mathcalD(beta y) \n quad nearrow \ny\nendalign*","category":"page"},{"location":"models/nomad/#Usage","page":"NOMAD","title":"Usage","text":"","category":"section"},{"location":"models/nomad/","page":"NOMAD","title":"NOMAD","text":"Let's try to learn the anti-derivative operator for","category":"page"},{"location":"models/nomad/","page":"NOMAD","title":"NOMAD","text":"u(x) = sin(alpha x)","category":"page"},{"location":"models/nomad/","page":"NOMAD","title":"NOMAD","text":"That is, we want to learn","category":"page"},{"location":"models/nomad/","page":"NOMAD","title":"NOMAD","text":"mathcalG  u rightarrow v ","category":"page"},{"location":"models/nomad/","page":"NOMAD","title":"NOMAD","text":"such that","category":"page"},{"location":"models/nomad/","page":"NOMAD","title":"NOMAD","text":"v(x) = fracdudx quad forall  x in 0 2pi  alpha in 05 1","category":"page"},{"location":"models/nomad/#Copy-pastable-code","page":"NOMAD","title":"Copy-pastable code","text":"","category":"section"},{"location":"models/nomad/","page":"NOMAD","title":"NOMAD","text":"using NeuralOperators, Lux, Random, Optimisers, Reactant\n\nusing CairoMakie, AlgebraOfGraphics\nset_aog_theme!()\nconst AoG = AlgebraOfGraphics\n\nrng = Random.default_rng()\nRandom.seed!(rng, 1234)\n\nxdev = reactant_device()\n\neval_points = 1\nbatch_size = 64\ndim_y = 1\nm = 32\n\nxrange = range(0, 2π; length=m) .|> Float32\nα = 0.5f0 .+ 0.5f0 .* rand(Float32, batch_size)\n\nu_data = zeros(Float32, m, batch_size)\ny_data = rand(rng, Float32, eval_points, batch_size) .* Float32(2π)\nv_data = zeros(Float32, eval_points, batch_size)\n\nfor i in 1:batch_size\n    u_data[:, i] .= sin.(α[i] .* xrange)\n    v_data[:, i] .= -inv(α[i]) .* cos.(α[i] .* y_data[:, i])\nend\n\nnomad = NOMAD(\n    Chain(Dense(m => 8, σ), Dense(8 => 8, σ), Dense(8 => 8 - eval_points)),\n    Chain(Dense(8 => 4, σ), Dense(4 => eval_points))\n)\n\nps, st = Lux.setup(rng, nomad) |> xdev;\nu_data = u_data |> xdev;\ny_data = y_data |> xdev;\nv_data = v_data |> xdev;\ndata = [((u_data, y_data), v_data)];\n\nfunction train!(model, ps, st, data; epochs=10)\n    losses = []\n    tstate = Training.TrainState(model, ps, st, Adam(0.001f0))\n    for _ in 1:epochs, (x, y) in data\n        (_, loss, _, tstate) = Training.single_train_step!(\n            AutoEnzyme(), MSELoss(), (x, y), tstate; return_gradients=Val(false)\n        )\n        push!(losses, Float32(loss))\n    end\n    return losses\nend\n\nlosses = train!(nomad, ps, st, data; epochs=1000)\n\ndraw(\n    AoG.data((; losses, iteration=1:length(losses))) *\n    mapping(:iteration => \"Iteration\", :losses => \"Loss (log10 scale)\") *\n    visual(Lines);\n    axis=(; yscale=log10),\n    figure=(; title=\"Using NOMAD to learn the anti-derivative operator\")\n)","category":"page"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Pre-Built-Architectures","page":"API Reference","title":"Pre-Built Architectures","text":"","category":"section"},{"location":"api/#NeuralOperators.NOMAD","page":"API Reference","title":"NeuralOperators.NOMAD","text":"NOMAD(approximator, decoder)\n\nConstructs a NOMAD from approximator and decoder architectures. Make sure the output from approximator combined with the coordinate dimension has compatible size for input to decoder\n\nArguments\n\napproximator: Lux network to be used as approximator net.\ndecoder: Lux network to be used as decoder net.\n\nReferences\n\n[1] Jacob H. Seidman and Georgios Kissas and Paris Perdikaris and George J. Pappas, \"NOMAD: Nonlinear Manifold Decoders for Operator Learning\", doi: https://arxiv.org/abs/2206.03551\n\nExample\n\njulia> approximator_net = Chain(Dense(8 => 32), Dense(32 => 32), Dense(32 => 16));\n\njulia> decoder_net = Chain(Dense(18 => 16), Dense(16 => 16), Dense(16 => 8));\n\njulia> nomad = NOMAD(approximator_net, decoder_net);\n\njulia> ps, st = Lux.setup(Xoshiro(), nomad);\n\njulia> u = rand(Float32, 8, 5);\n\njulia> y = rand(Float32, 2, 5);\n\njulia> size(first(nomad((u, y), ps, st)))\n(8, 5)\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralOperators.DeepONet","page":"API Reference","title":"NeuralOperators.DeepONet","text":"DeepONet(branch, trunk, additional)\n\nConstructs a DeepONet from a branch and trunk architectures. Make sure that both the nets output should have the same first dimension.\n\nArguments\n\nbranch: Lux network to be used as branch net.\ntrunk: Lux network to be used as trunk net.\n\nReferences\n\n[1] Lu Lu, Pengzhan Jin, George Em Karniadakis, \"DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators\", doi: https://arxiv.org/abs/1910.03193\n\nInput Output Dimensions\n\nConsider a transient 1D advection problem ∂ₜu + u ⋅ ∇u = 0, with an IC u(x,0) = g(x). We are given several (b = 200) instances of the IC, discretized at 50 points each, and want to query the solution for 100 different locations and times [0;1].\n\nThat makes the branch input of shape [50 x 200] and the trunk input of shape [2 x 100]. So, the input for the branch net is 50 and 100 for the trunk net.\n\nExample\n\njulia> branch_net = Chain(Dense(64 => 32), Dense(32 => 32), Dense(32 => 16));\n\njulia> trunk_net = Chain(Dense(1 => 8), Dense(8 => 8), Dense(8 => 16));\n\njulia> deeponet = DeepONet(branch_net, trunk_net);\n\njulia> ps, st = Lux.setup(Xoshiro(), deeponet);\n\njulia> u = rand(Float32, 64, 5);\n\njulia> y = rand(Float32, 1, 10);\n\njulia> size(first(deeponet((u, y), ps, st)))\n(10, 5)\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralOperators.FourierNeuralOperator","page":"API Reference","title":"NeuralOperators.FourierNeuralOperator","text":"FourierNeuralOperator(\n    σ=gelu;\n    chs::Dims{C}=(2, 64, 64, 64, 64, 64, 128, 1),\n    modes::Dims{M}=(16,),\n    kwargs...\n) where {C, M}\n\nThe Fourier neural operator is a operator learning model that uses a Fourier kernel to perform spectral convolutions. It is a promising operator for surrogate methods, and can be regarded as a physics operator.\n\nThe model is composed of a Dense layer to lift a (d + 1)-dimensional vector field to an n-dimensional vector field, an integral kernel operator which consists of four Fourier kernels, and two Dense layers to project data back to the scalar field of the space of interest.\n\nArguments\n\nσ: Activation function for all layers in the model.\n\nKeyword Arguments\n\nchs: A Tuple or Vector of the size of each of the 8 channels.\nmodes: The modes to be preserved. A tuple of length d, where d is the dimension of data.  For example, one-dimensional data would have a 1-element tuple, and two-dimensional data would have a 2-element tuple.\n\nExample\n\njulia> fno = FourierNeuralOperator(gelu; chs=(2, 64, 64, 128, 1), modes=(16,));\n\njulia> ps, st = Lux.setup(Xoshiro(), fno);\n\njulia> u = rand(Float32, 1024, 2, 5);\n\njulia> size(first(fno(u, ps, st)))\n(1024, 1, 5)\n\n\n\n\n\n","category":"type"},{"location":"api/#Building-blocks","page":"API Reference","title":"Building blocks","text":"","category":"section"},{"location":"api/#NeuralOperators.OperatorConv","page":"API Reference","title":"NeuralOperators.OperatorConv","text":"OperatorConv(\n    ch::Pair{<:Integer, <:Integer}, modes::Dims, tr::AbstractTransform;\n    init_weight=glorot_uniform\n)\n\nArguments\n\nch: A Pair of input and output channel size ch_in => ch_out, e.g. 64 => 64.\nmodes: The modes to be preserved. A tuple of length d, where d is the dimension of data.\ntr: The transform to operate the transformation.\n\nKeyword Arguments\n\ninit_weight: Initial function to initialize parameters.\n\nExample\n\njulia> OperatorConv(2 => 5, (16,), FourierTransform{ComplexF32}((16,)));\n\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralOperators.SpectralConv","page":"API Reference","title":"NeuralOperators.SpectralConv","text":"SpectralConv(args...; kwargs...)\n\nConstruct a OperatorConv with FourierTransform{ComplexF32} as the transform. See OperatorConv for the individual arguments.\n\nExample\n\njulia> SpectralConv(2 => 5, (16,));\n\n\n\n\n\n\n","category":"function"},{"location":"api/#NeuralOperators.OperatorKernel","page":"API Reference","title":"NeuralOperators.OperatorKernel","text":"OperatorKernel(\n    ch::Pair{<:Integer, <:Integer}, modes::Dims, transform::AbstractTransform,\n    act=identity; kwargs...\n)\n\nArguments\n\nch: A Pair of input and output channel size ch_in => ch_out, e.g. 64 => 64.\nmodes: The modes to be preserved. A tuple of length d, where d is the dimension of data.\ntransform: The transform to operate the transformation.\nact: Activation function.\n\nAll the keyword arguments are passed to the OperatorConv constructor.\n\nExample\n\njulia> OperatorKernel(2 => 5, (16,), FourierTransform{ComplexF64}((16,)));\n\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralOperators.SpectralKernel","page":"API Reference","title":"NeuralOperators.SpectralKernel","text":"SpectralKernel(args...; kwargs...)\n\nConstruct a OperatorKernel with FourierTransform{ComplexF32} as the transform. See OperatorKernel for the individual arguments.\n\nExample\n\njulia> SpectralKernel(2 => 5, (16,));\n\n\n\n\n\n\n","category":"function"},{"location":"api/#NeuralOperators.GridEmbedding","page":"API Reference","title":"NeuralOperators.GridEmbedding","text":"GridEmbedding(grid_boundaries::Vector{<:Tuple{<:Real,<:Real}})\n\nAppends a uniform grid embedding to the input data along the penultimate dimension.\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralOperators.ComplexDecomposedLayer","page":"API Reference","title":"NeuralOperators.ComplexDecomposedLayer","text":"ComplexDecomposedLayer(layer::AbstractLuxLayer)\n\nDecomposes complex activations into real and imaginary parts and applies the given layer to each component separately, and then recombines the real and imaginary parts.\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralOperators.SoftGating","page":"API Reference","title":"NeuralOperators.SoftGating","text":"SoftGating(chs::Integer, ndims::Integer; kwargs...)\n\nConstructs a wrapper over Scale with dims = (ntuple(Returns(1), ndims)..., chs). All keyword arguments are passed to the Scale constructor.\n\n\n\n\n\n","category":"type"},{"location":"api/#Transform-API","page":"API Reference","title":"Transform API","text":"","category":"section"},{"location":"api/#NeuralOperators.AbstractTransform","page":"API Reference","title":"NeuralOperators.AbstractTransform","text":"AbstractTransform\n\nInterface\n\nBase.ndims(<:AbstractTransform): N dims of modes\ntransform(<:AbstractTransform, x::AbstractArray): Apply the transform to x\ntruncate_modes(<:AbstractTransform, x_transformed::AbstractArray): Truncate modes that contribute to the noise\ninverse(<:AbstractTransform, x_transformed::AbstractArray): Apply the inverse transform to x_transformed\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralOperators.FourierTransform","page":"API Reference","title":"NeuralOperators.FourierTransform","text":"FourierTransform{T}(modes, shift::Bool=false)\n\nA concrete implementation of AbstractTransform for Fourier transforms.\n\nIf shift is true, we apply a fftshift before truncating the modes.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/poisson_equation/#DeepONet-for-1D-Poisson-Equation","page":"Solving Poisson Equation","title":"DeepONet for 1D Poisson Equation","text":"","category":"section"},{"location":"tutorials/poisson_equation/#Mathematical-Formulation","page":"Solving Poisson Equation","title":"Mathematical Formulation","text":"","category":"section"},{"location":"tutorials/poisson_equation/#Problem-Statement","page":"Solving Poisson Equation","title":"Problem Statement","text":"","category":"section"},{"location":"tutorials/poisson_equation/","page":"Solving Poisson Equation","title":"Solving Poisson Equation","text":"We consider the one-dimensional Poisson equation on the domain Omega = 01:","category":"page"},{"location":"tutorials/poisson_equation/","page":"Solving Poisson Equation","title":"Solving Poisson Equation","text":"-fracd^2u(x)dx^2 = f(x) quad x in 01","category":"page"},{"location":"tutorials/poisson_equation/","page":"Solving Poisson Equation","title":"Solving Poisson Equation","text":"subject to homogeneous Dirichlet boundary conditions:","category":"page"},{"location":"tutorials/poisson_equation/","page":"Solving Poisson Equation","title":"Solving Poisson Equation","text":"u(0) = u(1) = 0","category":"page"},{"location":"tutorials/poisson_equation/","page":"Solving Poisson Equation","title":"Solving Poisson Equation","text":"In our specific case, the forcing function f(x) is parameterized by alpha:","category":"page"},{"location":"tutorials/poisson_equation/","page":"Solving Poisson Equation","title":"Solving Poisson Equation","text":"f(x) = alpha sin(pi x)","category":"page"},{"location":"tutorials/poisson_equation/#Analytical-Solution","page":"Solving Poisson Equation","title":"Analytical Solution","text":"","category":"section"},{"location":"tutorials/poisson_equation/","page":"Solving Poisson Equation","title":"Solving Poisson Equation","text":"u(x) = fracalphapi^2 sin(pi x)","category":"page"},{"location":"tutorials/poisson_equation/","page":"Solving Poisson Equation","title":"Solving Poisson Equation","text":"The DeepONet architecture consists of:","category":"page"},{"location":"tutorials/poisson_equation/","page":"Solving Poisson Equation","title":"Solving Poisson Equation","text":"Branch network: Processes the discretized input function f(x)\nTrunk network: Processes the spatial coordinates x\nOutput: u(x) approx sum_k=1^p b_k(f) t_k(x), where b_k are outputs from the branch network and t_k are outputs from the trunk network","category":"page"},{"location":"tutorials/poisson_equation/#Implementation","page":"Solving Poisson Equation","title":"Implementation","text":"","category":"section"},{"location":"tutorials/poisson_equation/","page":"Solving Poisson Equation","title":"Solving Poisson Equation","text":"using NeuralOperators, Lux, Random, Optimisers, Reactant, Statistics\n\nusing CairoMakie, AlgebraOfGraphics\nconst AoG = AlgebraOfGraphics\nAoG.set_aog_theme!()\n\nconst cdev = cpu_device()\nconst xdev = reactant_device(; force=true)\n\nrng = Random.default_rng()\nRandom.seed!(rng, 42)\n\n# Problem setup\nm = 64\ndata_size = 128\nxrange = Float32.(range(0, 1; length=m))\n\nforcing_function(x, α) = α * sinpi.(x)\npoisson_solution(x, α) = (α / Float32(π)^2) * sinpi.(x)\n\n# Generate training data\nα_train = 0.5f0 .+ 0.8f0 .* rand(Float32, data_size)  # α ∈ [0.5, 1.3]\nf_data = stack(Base.Fix1(forcing_function, xrange), α_train)\nu_data = stack(Base.Fix1(poisson_solution, xrange), α_train)\nx_data = reshape(xrange, 1, m)\nmax_u = maximum(abs.(u_data))\nu_data ./= max_u\n\n# Define DeepONet\ndeeponet = DeepONet(\n    Chain(Dense(m => 64, tanh), Dense(64 => 64, tanh), Dense(64 => 32, tanh)),  # Branch\n    Chain(Dense(1 => 32, tanh), Dense(32 => 32, tanh))                          # Trunk\n)\n\nps, st = Lux.setup(Random.default_rng(), deeponet) |> xdev;\n\n# Training\nfunction train_model!(model, ps, st, data; epochs=3000)\n    train_state = Training.TrainState(model, ps, st, Adam(0.001f0))\n    losses = Float32[]\n\n    for epoch in 1:epochs\n        _, loss, _, train_state = Training.single_train_step!(\n            AutoEnzyme(), MSELoss(), data, train_state; return_gradients=Val(false)\n        )\n        epoch % 500 == 0 && println(\"Epoch: $epoch, Loss: $loss\")\n        push!(losses, loss)\n    end\n\n    return train_state.parameters, train_state.states, losses\nend\n\nf_data = f_data |> xdev;\nx_data = x_data |> xdev;\nu_data = u_data |> xdev;\ndata = ((f_data, x_data), u_data)\nps_trained, st_trained, losses = train_model!(deeponet, ps, st, data)\n\n# Prediction function\nfunction predict(model, f_input, x_input, ps, st)\n    pred, _ = model((f_input, x_input), ps, st)\n    return vec(pred .* max_u)\nend\n\ncompiled_predict_fn = Reactant.with_config(; dot_general_precision=PrecisionConfig.HIGH) do\n    @compile predict(deeponet, f_data[:, 1:1], x_data, ps_trained, st_trained)\nend\n\n# Testing and visualization\nbegin\n    results = Float32[]\n    labels = AbstractString[]\n    abs_errors = Float32[]\n    x_values = Float32[]\n    x_values2 = Float32[]\n    alphas = AbstractString[]\n    alphas2 = AbstractString[]\n\n    for (i, α) in enumerate([0.6f0, 0.8f0, 1.2f0])\n        f_test = reshape(forcing_function(xrange, α), :, 1)\n        u_pred = compiled_predict_fn(\n            deeponet, xdev(f_test), x_data, ps_trained, st_trained\n        ) |> cdev\n        u_true = reshape(poisson_solution(xrange, α), :, 1)\n\n        l2_error = sqrt(mean(abs2, u_pred .- u_true))\n        rel_error = l2_error / sqrt(mean(abs2, u_true)) * 100\n\n        text = L\"$ \\alpha = %$(α) $ (Rel. Error: $ %$(round(rel_error, digits=2))% $)\"\n\n        append!(results, vec(u_pred))\n        append!(labels, repeat([\"Predictions\"], length(xrange)))\n        append!(results, vec(u_true))\n        append!(labels, repeat([\"Ground Truth\"], length(xrange)))\n        append!(x_values, repeat(vec(xrange), 2))\n        append!(alphas, repeat([text], length(xrange) * 2))\n\n        append!(abs_errors, abs.(vec(u_pred .- u_true)))\n        append!(x_values2, vec(xrange))\n        append!(alphas2, repeat([text], length(xrange)))\n    end\nend\n\nplot_data = (; results, abs_errors, x_values, alphas, labels, x_values2, alphas2)\n\nbegin\n    fig = Figure(;\n        size=(1024, 512),\n        title=\"DeepONet Results for 1D Poisson Equation\",\n        titlesize=25\n    )\n\n    axis_common = (;\n        xlabelsize=20, ylabelsize=20, titlesize=20, xticklabelsize=20, yticklabelsize=20\n    )\n\n    axs1 = draw!(\n        fig[1, 1],\n        AoG.data(plot_data) *\n        mapping(\n            :x_values => L\"x\",\n            :results => L\"u(x)\";\n            color=:labels => \"\",\n            col=:alphas => \"\",\n            linestyle=:labels => \"\",\n        ) *\n        visual(Lines; linewidth=4),\n        scales(; Color=(; palette=[:orange, :blue]), LineStyle = (; palette = [:solid, :dash]));\n        axis=merge(axis_common, (; xlabel=\"\")),\n    )\n    for ax in axs1\n        hidexdecorations!(ax; grid=false)\n    end\n\n    axislegend(\n        axs1[1, 1].axis,\n        [\n            LineElement(; linestyle=:solid, color=:orange),\n            LineElement(; linestyle=:dash, color=:blue),\n        ],\n        [\"Ground Truth\", \"Predictions\"],\n        labelsize=20,\n    )\n\n    axs2 = draw!(\n        fig[2, 1],\n        AoG.data(plot_data) *\n        mapping(\n            :x_values2 => L\"x\",\n            :abs_errors => L\"|u(x) - u(x_{true})|\";\n            col=:alphas2 => \"\",\n        ) *\n        visual(Lines; linewidth=4, color=:green);\n        axis=merge(axis_common, (; titlevisible=false)),\n    )\n\n    fig\nend","category":"page"},{"location":"tutorials/burgers_deeponet/#Burgers-Equation-using-DeepONet","page":"DeepONet","title":"Burgers Equation using DeepONet","text":"","category":"section"},{"location":"tutorials/burgers_deeponet/#Data-Loading","page":"DeepONet","title":"Data Loading","text":"","category":"section"},{"location":"tutorials/burgers_deeponet/","page":"DeepONet","title":"DeepONet","text":"using DataDeps, MAT, MLUtils\nusing PythonCall, CondaPkg # For `gdown`\nusing Printf\n\nconst gdown = pyimport(\"gdown\")\n\nregister(\n    DataDep(\n    \"Burgers\",\n    \"\"\"\n    Burgers' equation dataset from\n    [fourier_neural_operator](https://github.com/zongyi-li/fourier_neural_operator)\n\n    mapping between initial conditions to the solutions at the last point of time \\\n    evolution in some function space.\n\n    u(x,0) -> u(x, time_end):\n\n      * `a`: initial conditions u(x,0)\n      * `u`: solutions u(x,t_end)\n    \"\"\",\n    \"https://drive.google.com/uc?id=16a8od4vidbiNR3WtaBPCSZ0T3moxjhYe\",\n    \"9cbbe5070556c777b1ba3bacd49da5c36ea8ed138ba51b6ee76a24b971066ecd\";\n    fetch_method=(url,\n        local_dir) -> begin\n        pyconvert(String, gdown.download(url, joinpath(local_dir, \"Burgers_R10.zip\")))\n    end,\n    post_fetch_method=unpack\n)\n)\n\nfilepath = joinpath(datadep\"Burgers\", \"burgers_data_R10.mat\")\n\nconst N = 2048\nconst Δsamples = 2^3\nconst grid_size = div(2^13, Δsamples)\nconst T = Float32\n\nfile = matopen(filepath)\nx_data = reshape(T.(collect(read(file, \"a\")[1:N, 1:Δsamples:end])), N, :)\ny_data = reshape(T.(collect(read(file, \"u\")[1:N, 1:Δsamples:end])), N, :)\nclose(file)\n\nx_data = permutedims(x_data, (2, 1))\ny_data = permutedims(y_data, (2, 1))\ngrid = reshape(collect(T, range(0, 1; length=grid_size)), 1, :)","category":"page"},{"location":"tutorials/burgers_deeponet/#Model","page":"DeepONet","title":"Model","text":"","category":"section"},{"location":"tutorials/burgers_deeponet/","page":"DeepONet","title":"DeepONet","text":"using Lux, NeuralOperators, Optimisers,  Random, Reactant\n\nconst cdev = cpu_device()\nconst xdev = reactant_device(; force=true)\n\ndeeponet = DeepONet(;\n    branch=(size(x_data, 1), ntuple(Returns(32), 5)...),\n    trunk=(size(grid, 1), ntuple(Returns(32), 5)...),\n    branch_activation=gelu,\n    trunk_activation=gelu\n)\nps, st = Lux.setup(Random.default_rng(), deeponet) |> xdev;","category":"page"},{"location":"tutorials/burgers_deeponet/#Training","page":"DeepONet","title":"Training","text":"","category":"section"},{"location":"tutorials/burgers_deeponet/","page":"DeepONet","title":"DeepONet","text":"x_data_dev = x_data |> xdev;\ny_data_dev = y_data |> xdev;\ngrid_dev = grid |> xdev;\n\nfunction train_model!(model, ps, st, data; epochs=5000)\n    train_state = Training.TrainState(model, ps, st, Adam(0.0001f0))\n\n    for epoch in 1:epochs\n        (_, loss, _, train_state) = Training.single_train_step!(\n            AutoEnzyme(), MAELoss(), data, train_state; return_gradients=Val(false)\n        )\n\n        if epoch % 100 == 1 || epoch == epochs\n            @printf(\"Epoch %d: loss = %.6e\\n\", epoch, loss)\n        end\n    end\n\n    return train_state.parameters, train_state.states\nend\n\n(ps_trained, st_trained) = train_model!(\n    deeponet, ps, st, ((x_data_dev, grid_dev), y_data_dev)\n)\nnothing #hide","category":"page"},{"location":"tutorials/burgers_deeponet/#Plotting","page":"DeepONet","title":"Plotting","text":"","category":"section"},{"location":"tutorials/burgers_deeponet/","page":"DeepONet","title":"DeepONet","text":"using CairoMakie, AlgebraOfGraphics\nconst AoG = AlgebraOfGraphics\nAoG.set_aog_theme!()\n\npred = first(\n    Reactant.with_config(;\n        convolution_precision=PrecisionConfig.HIGH,\n        dot_general_precision=PrecisionConfig.HIGH,\n    ) do\n        @jit(deeponet((x_data_dev, grid_dev), ps_trained, st_trained))\n    end\n) |> cdev\n\ndata_sequence, sequence, repeated_grid, label = Float32[], Int[], Float32[], String[]\nfor i in 1:16\n    append!(repeated_grid, vcat(vec(grid), vec(grid)))\n    append!(sequence, repeat([i], grid_size * 2))\n    append!(label, repeat([\"Ground Truth\"], grid_size))\n    append!(label, repeat([\"Predictions\"], grid_size))\n    append!(data_sequence, vec(y_data[:, i]))\n    append!(data_sequence, vec(pred[:, i]))\nend\nplot_data = (; data_sequence, sequence, repeated_grid, label)\n\ndraw(\n    AoG.data(plot_data) *\n    mapping(\n        :repeated_grid => L\"x\",\n        :data_sequence => L\"u(x)\";\n        color=:label => \"\",\n        layout=:sequence => nonnumeric,\n        linestyle=:label => \"\",\n    ) *\n    visual(Lines; linewidth=4),\n    scales(; Color=(; palette=:tab10), LineStyle = (; palette = [:solid, :dash]));\n    figure=(;\n        size=(1024, 1024),\n        title=\"Using DeepONet to solve the Burgers equation\",\n        titlesize=25,\n    ),\n    axis=(; xlabelsize=25, ylabelsize=25),\n    legend=(; label=L\"u(x)\", position=:bottom, labelsize=20),\n)","category":"page"},{"location":"models/fno/#Fourier-Neural-Operators-(FNOs)","page":"FNO","title":"Fourier Neural Operators (FNOs)","text":"","category":"section"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"FNOs are a subclass of Neural Operators that learn the learn the kernel Kappa_theta, parameterized on theta between function spaces:","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"(Kappa_thetau)(x) = int_D kappa_theta(a(x) a(y) x y) dy  quad forall x in D","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"The kernel makes up a block v_t(x) which passes the information to the next block as:","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"v^(t+1)(x) = sigma((W^(t)v^(t) + Kappa^(t)v^(t))(x))","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"FNOs choose a specific kernel kappa(xy) = kappa(x-y), converting the kernel into a convolution operation, which can be efficiently computed in the fourier domain.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"beginalign*\n(Kappa_thetau)(x)\n= int_D kappa_theta(x - y) dy  quad forall x in D\n= mathcalF^-1(mathcalF(kappa_theta) mathcalF(u))(x) quad forall x in D\nendalign*","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"where mathcalF denotes the fourier transform. Usually, not all the modes in the frequency domain are used with the higher modes often being truncated.","category":"page"},{"location":"models/fno/#Usage","page":"FNO","title":"Usage","text":"","category":"section"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"Let's try to learn the anti-derivative operator for","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"u(x) = sin(alpha x)","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"That is, we want to learn","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"mathcalG  u rightarrow v ","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"such that","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"v(x) = fracdudx quad forall  x in 0 2pi  alpha in 05 1","category":"page"},{"location":"models/fno/#Copy-pastable-code","page":"FNO","title":"Copy-pastable code","text":"","category":"section"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"<details>\n\n<summary> Click here to see copy-pastable code for this example! </summary>\n","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"using NeuralOperators, Lux, Random, Optimisers, Reactant\n\nusing CairoMakie, AlgebraOfGraphics\nset_aog_theme!()\nconst AoG = AlgebraOfGraphics\n\nrng = Random.default_rng()\nRandom.seed!(rng, 1234)\n\nxdev = reactant_device()\n\nbatch_size = 128\nm = 32\n\nxrange = range(0, 2π; length=m) .|> Float32;\nu_data = zeros(Float32, m, 1, batch_size);\nα = 0.5f0 .+ 0.5f0 .* rand(Float32, batch_size);\nv_data = zeros(Float32, m, 1, batch_size);\n\nfor i in 1:batch_size\n    u_data[:, 1, i] .= sin.(α[i] .* xrange)\n    v_data[:, 1, i] .= -inv(α[i]) .* cos.(α[i] .* xrange)\nend\n\nfno = FourierNeuralOperator(gelu; chs=(1, 64, 64, 128, 1), modes=(16,))\n\nps, st = Lux.setup(rng, fno) |> xdev;\nu_data = u_data |> xdev;\nv_data = v_data |> xdev;\ndata = [(u_data, v_data)];\n\nfunction train!(model, ps, st, data; epochs=10)\n    losses = []\n    tstate = Training.TrainState(model, ps, st, Adam(0.003f0))\n    for _ in 1:epochs, (x, y) in data\n        (_, loss, _, tstate) = Training.single_train_step!(\n            AutoEnzyme(), MSELoss(), (x, y), tstate; return_gradients=Val(false)\n        )\n        push!(losses, Float32(loss))\n    end\n    return losses\nend\n\nlosses = train!(fno, ps, st, data; epochs=1000)\n\ndraw(\n    AoG.data((; losses, iteration=1:length(losses))) *\n    mapping(:iteration => \"Iteration\", :losses => \"Loss (log10 scale)\") *\n    visual(Lines);\n    axis=(; yscale=log10),\n    figure=(; title=\"Using Fourier Neural Operator to learn the anti-derivative operator\")\n)","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"</details>","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"using NeuralOperators, Lux, Random, Optimisers, Reactant","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"We will use Reactant.jl to accelerate the training process.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"xdev = reactant_device()","category":"page"},{"location":"models/fno/#Constructing-training-data","page":"FNO","title":"Constructing training data","text":"","category":"section"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"First, we construct our training data.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"rng = Random.default_rng()","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"batch_size is the number of observations.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"batch_size = 128","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"m is the length of a single observation, you can also interpret this as the size of the grid we're evaluating our function on.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"m = 32","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"We instantiate the domain that the function operates on as a range from 0 to 2π, whose length is the grid size.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"xrange = range(0, 2π; length=m) .|> Float32;\nnothing #hide","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"Each value in the array here, α, will be the multiplicative factor on the input to the sine function.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"α = 0.5f0 .+ 0.5f0 .* rand(Float32, batch_size);\nnothing #hide","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"Now, we create our data arrays. We are storing all of the training data in a single array, in order to batch process them more efficiently.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"u_data = zeros(Float32, m, 1, batch_size);\nv_data = zeros(Float32, m, 1, batch_size);\nnothing #hide","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"and fill the data arrays with values. Here, u_data is","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"for i in 1:batch_size\n    u_data[:, 1, i] .= sin.(α[i] .* xrange)\n    v_data[:, 1, i] .= -inv(α[i]) .* cos.(α[i] .* xrange)\nend","category":"page"},{"location":"models/fno/#Creating-the-model","page":"FNO","title":"Creating the model","text":"","category":"section"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"Finally, we get to the model itself. We instantiate a FourierNeuralOperator and provide it several parameters.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"The first argument is the \"activation function\" for each neuron.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"The keyword arguments are:","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"chs is a tuple, representing the layer sizes for each layer.\nmodes is a 1-tuple, where the number represents the number of Fourier modes that are preserved, and the size of the tuple represents the number of dimensions.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"fno = FourierNeuralOperator(\n    gelu;                    # activation function\n    chs=(1, 64, 64, 128, 1), # channel weights\n    modes=(16,),             # number of Fourier modes to retain\n)","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"Now, we set up the model. This function returns two things, a set of parameters and a set of states. Since the operator is \"stateless\", the states are empty and will remain so. The parameters are the weights of the neural network, and we will be modifying them in the training loop.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"ps, st = Lux.setup(rng, fno) |> xdev;\nnothing #hide","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"We construct data as a vector of tuples (input, output). These are pre-batched, but for example if we had a lot of training data, we could dynamically load it, or create multiple batches.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"u_data = u_data |> xdev;\nv_data = v_data |> xdev;\ndata = [(u_data, v_data)];\nnothing #hide","category":"page"},{"location":"models/fno/#Training-the-model","page":"FNO","title":"Training the model","text":"","category":"section"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"Now, we create a function to train the model. An \"epoch\" is basically a run over all input data, and the more epochs we have, the better the neural network gets!","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"function train!(model, ps, st, data; epochs=10)\n    # The `losses` array is used only for visualization,\n    # you don't actually need it to train.\n    losses = []\n    # Initialize a training state and an optimizer (Adam, in this case).\n    tstate = Training.TrainState(model, ps, st, Adam(0.003f0))\n    # Loop over epochs, then loop over each batch of training data, and step into the\n    # training:\n    for _ in 1:epochs\n        for (x, y) in data\n            (_, loss, _, tstate) = Training.single_train_step!(\n                AutoEnzyme(), MSELoss(), (x, y), tstate; return_gradients=Val(false)\n            )\n            push!(losses, Float32(loss))\n        end\n    end\n    return losses, tstate.parameters, tstate.states\nend","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"Now we train our model!","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"losses, ps, st = @time train!(fno, ps, st, data; epochs=500)","category":"page"},{"location":"models/fno/#Applying-the-model","page":"FNO","title":"Applying the model","text":"","category":"section"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"Let's try to actually apply this model using some input data.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"input_data = u_data[:, 1, 1]","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"This is our input data. It's currently one-dimensional, but our neural network expects input in batched form, so we simply reshape it (a no-cost operation) to a 3d array with singleton dimensions.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"reshaped_input = reshape(input_data, length(input_data), 1, 1)","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"Now we can pass this to Lux.apply (@jit is used to run the function with Reactant.jl):","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"output_data, st = @jit Lux.apply(fno, reshaped_input, ps, st)","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"and plot it:","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"using CairoMakie, AlgebraOfGraphics\nconst AoG = AlgebraOfGraphics\nAoG.set_aog_theme!()\n\nf, a, p = lines(dropdims(Array(reshaped_input); dims=(2, 3)); label=\"u\")\nlines!(a, dropdims(Array(output_data); dims=(2, 3)); label=\"Predicted\")\nlines!(a, Array(v_data)[:, 1, 1]; label=\"Expected\")\naxislegend(a)\n# Compute the absolute error and plot that too,\n# on a separate axis.\nabsolute_error = Array(v_data)[:, 1, 1] .- dropdims(Array(output_data); dims=(2, 3))\na2, p2 = lines(f[2, 1], absolute_error; axis=(; ylabel=\"Error\"))\nrowsize!(f.layout, 2, Aspect(1, 1 / 8))\nlinkxaxes!(a, a2)\nf","category":"page"},{"location":"#NeuralOperators","page":"NeuralOperators.jl","title":"NeuralOperators","text":"","category":"section"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"NeuralOperators.jl is a package written in Julia to provide the architectures for learning mapping between function spaces, and learning grid invariant solution of PDEs.","category":"page"},{"location":"#Installation","page":"NeuralOperators.jl","title":"Installation","text":"","category":"section"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"On Julia 1.10+, you can install NeuralOperators.jl by running","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"import Pkg\nPkg.add(\"NeuralOperators\")","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"Currently provided operator architectures are :","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"Fourier Neural Operators (FNOs)\nDeepONets\nNonlinear Manifold Decoders for Operator Learning (NOMADs)","category":"page"},{"location":"#Reproducibility","page":"NeuralOperators.jl","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"</details>","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"</details>","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"using Pkg # hide\nPkg.status(; mode=PKGMODE_MANIFEST) # hide","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"</details>","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"using TOML\nusing Markdown\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink_manifest = \"https://github.com/SciML/\" *\n                name *\n                \".jl/tree/gh-pages/v\" *\n                version *\n                \"/assets/Manifest.toml\"\nlink_project = \"https://github.com/SciML/\" *\n               name *\n               \".jl/tree/gh-pages/v\" *\n               version *\n               \"/assets/Project.toml\"\nMarkdown.parse(\"\"\"You can also download the\n[manifest]($link_manifest)\nfile and the\n[project]($link_project)\nfile.\n\"\"\")","category":"page"},{"location":"models/deeponet/#DeepONets","page":"DeepONet","title":"DeepONets","text":"","category":"section"},{"location":"models/deeponet/","page":"DeepONet","title":"DeepONet","text":"DeepONets are another class of networks that learn the mapping between two function spaces by encoding the input function space and the location of the output space. The latent code of the input space is then projected on the location laten code to give the output. This allows the network to learn the mapping between two functions defined on different spaces.","category":"page"},{"location":"models/deeponet/","page":"DeepONet","title":"DeepONet","text":"beginalign*\nu(y) xrightarrowtextbranch   b \n quad searrow\nquad quad mathcalG_theta u(y) = sum_k b_k t_k \n  quad nearrow \ny   xrightarrowtexttrunk    t\nendalign*","category":"page"},{"location":"models/deeponet/#Usage","page":"DeepONet","title":"Usage","text":"","category":"section"},{"location":"models/deeponet/","page":"DeepONet","title":"DeepONet","text":"Let's try to learn the anti-derivative operator for","category":"page"},{"location":"models/deeponet/","page":"DeepONet","title":"DeepONet","text":"u(x) = sin(alpha x)","category":"page"},{"location":"models/deeponet/","page":"DeepONet","title":"DeepONet","text":"That is, we want to learn","category":"page"},{"location":"models/deeponet/","page":"DeepONet","title":"DeepONet","text":"mathcalG  u rightarrow v ","category":"page"},{"location":"models/deeponet/","page":"DeepONet","title":"DeepONet","text":"such that","category":"page"},{"location":"models/deeponet/","page":"DeepONet","title":"DeepONet","text":"v(x) = fracdudx quad forall  x in 0 2pi  alpha in 05 1","category":"page"},{"location":"models/deeponet/#Copy-pastable-code","page":"DeepONet","title":"Copy-pastable code","text":"","category":"section"},{"location":"models/deeponet/","page":"DeepONet","title":"DeepONet","text":"using NeuralOperators, Lux, Random, Optimisers, Reactant\n\nusing CairoMakie, AlgebraOfGraphics\nset_aog_theme!()\nconst AoG = AlgebraOfGraphics\n\nrng = Random.default_rng()\nRandom.seed!(rng, 1234)\n\nxdev = reactant_device()\n\neval_points = 1\nbatch_size = 64\ndim_y = 1\nm = 32\n\nxrange = range(0, 2π; length=m) .|> Float32\nα = 0.5f0 .+ 0.5f0 .* rand(Float32, batch_size)\n\nu_data = zeros(Float32, m, batch_size)\ny_data = rand(rng, Float32, 1, eval_points) .* Float32(2π)\nv_data = zeros(Float32, eval_points, batch_size)\n\nfor i in 1:batch_size\n    u_data[:, i] .= sin.(α[i] .* xrange)\n    v_data[:, i] .= -inv(α[i]) .* cos.(α[i] .* y_data[1, :])\nend\n\ndeeponet = DeepONet(\n    Chain(Dense(m => 8, σ), Dense(8 => 8, σ), Dense(8 => 8, σ)),\n    Chain(Dense(1 => 4, σ), Dense(4 => 8, σ))\n)\n\nps, st = Lux.setup(rng, deeponet) |> xdev;\n\nu_data = u_data |> xdev;\ny_data = y_data |> xdev;\nv_data = v_data |> xdev;\ndata = [((u_data, y_data), v_data)];\n\nfunction train!(model, ps, st, data; epochs=10)\n    losses = []\n    tstate = Training.TrainState(model, ps, st, Adam(0.001f0))\n    for _ in 1:epochs, (x, y) in data\n        (_, loss, _, tstate) = Training.single_train_step!(\n            AutoEnzyme(), MSELoss(), (x, y), tstate; return_gradients=Val(false)\n        )\n        push!(losses, Float32(loss))\n    end\n    return losses\nend\n\nlosses = train!(deeponet, ps, st, data; epochs=1000)\n\ndraw(\n    AoG.data((; losses, iteration=1:length(losses))) *\n    mapping(:iteration => \"Iteration\", :losses => \"Loss (log10 scale)\") *\n    visual(Lines);\n    axis=(; yscale=log10),\n    figure=(; title=\"Using DeepONet to learn the anti-derivative operator\")\n)","category":"page"},{"location":"tutorials/burgers_fno/#Burgers-Equation-using-Fourier-Neural-Operator","page":"FNO","title":"Burgers Equation using Fourier Neural Operator","text":"","category":"section"},{"location":"tutorials/burgers_fno/#Data-Loading","page":"FNO","title":"Data Loading","text":"","category":"section"},{"location":"tutorials/burgers_fno/","page":"FNO","title":"FNO","text":"using DataDeps, MAT, MLUtils\nusing PythonCall, CondaPkg # For `gdown`\nusing Printf\n\nconst gdown = pyimport(\"gdown\")\n\nregister(\n    DataDep(\n    \"Burgers\",\n    \"\"\"\n    Burgers' equation dataset from\n    [fourier_neural_operator](https://github.com/zongyi-li/fourier_neural_operator)\n\n    mapping between initial conditions to the solutions at the last point of time \\\n    evolution in some function space.\n\n    u(x,0) -> u(x, time_end):\n\n      * `a`: initial conditions u(x,0)\n      * `u`: solutions u(x,t_end)\n    \"\"\",\n    \"https://drive.google.com/uc?id=16a8od4vidbiNR3WtaBPCSZ0T3moxjhYe\",\n    \"9cbbe5070556c777b1ba3bacd49da5c36ea8ed138ba51b6ee76a24b971066ecd\";\n    fetch_method=(url,\n        local_dir) -> begin\n        pyconvert(String, gdown.download(url, joinpath(local_dir, \"Burgers_R10.zip\")))\n    end,\n    post_fetch_method=unpack\n)\n)\n\nfilepath = joinpath(datadep\"Burgers\", \"burgers_data_R10.mat\")\n\nconst N = 2048\nconst Δsamples = 2^3\nconst grid_size = div(2^13, Δsamples)\nconst T = Float32\n\nfile = matopen(filepath)\nx_data = reshape(T.(collect(read(file, \"a\")[1:N, 1:Δsamples:end])), N, :)\ny_data = reshape(T.(collect(read(file, \"u\")[1:N, 1:Δsamples:end])), N, :)\nclose(file)\n\nx_data = reshape(permutedims(x_data, (2, 1)), grid_size, 1, N);\ny_data = reshape(permutedims(y_data, (2, 1)), grid_size, 1, N);","category":"page"},{"location":"tutorials/burgers_fno/#Model","page":"FNO","title":"Model","text":"","category":"section"},{"location":"tutorials/burgers_fno/","page":"FNO","title":"FNO","text":"using Lux, NeuralOperators, Optimisers,  Random, Reactant\n\nconst cdev = cpu_device()\nconst xdev = reactant_device(; force=true)\n\nfno = FourierNeuralOperator(\n    (16,), 2, 1, 32; activation=gelu, stabilizer=tanh, shift=true\n)\nps, st = Lux.setup(Random.default_rng(), fno) |> xdev;","category":"page"},{"location":"tutorials/burgers_fno/#Training","page":"FNO","title":"Training","text":"","category":"section"},{"location":"tutorials/burgers_fno/","page":"FNO","title":"FNO","text":"dataloader = DataLoader((x_data, y_data); batchsize=128, shuffle=true) |> xdev;\n\nfunction train_model!(model, ps, st, dataloader; epochs=1000)\n    train_state = Training.TrainState(model, ps, st, Adam(0.0001f0))\n\n    for epoch in 1:epochs\n        loss = -Inf\n        for data in dataloader\n            (_, loss, _, train_state) = Training.single_train_step!(\n                AutoEnzyme(), MAELoss(), data, train_state; return_gradients=Val(false)\n            )\n        end\n\n        if epoch % 100 == 1 || epoch == epochs\n            @printf(\"Epoch %d: loss = %.6e\\n\", epoch, loss)\n        end\n    end\n\n    return train_state.parameters, train_state.states\nend\n\nps_trained, st_trained = train_model!(fno, ps, st, dataloader)\nnothing #hide","category":"page"},{"location":"tutorials/burgers_fno/#Plotting","page":"FNO","title":"Plotting","text":"","category":"section"},{"location":"tutorials/burgers_fno/","page":"FNO","title":"FNO","text":"using CairoMakie, AlgebraOfGraphics\nconst AoG = AlgebraOfGraphics\nAoG.set_aog_theme!()\n\nx_data_dev = x_data |> xdev;\ny_data_dev = y_data |> xdev;\n\ngrid = range(0, 1; length=grid_size)\npred = first(\n    Reactant.with_config(;\n        convolution_precision=PrecisionConfig.HIGH,\n        dot_general_precision=PrecisionConfig.HIGH,\n    ) do\n        @jit(fno(x_data_dev, ps_trained, st_trained))\n    end\n) |> cdev\n\ndata_sequence, sequence, repeated_grid, label = Float32[], Int[], Float32[], String[]\nfor i in 1:16\n    append!(repeated_grid, repeat(grid, 2))\n    append!(sequence, repeat([i], grid_size * 2))\n    append!(label, repeat([\"Ground Truth\"], grid_size))\n    append!(label, repeat([\"Predictions\"], grid_size))\n    append!(data_sequence, vec(y_data[:, 1, i]))\n    append!(data_sequence, vec(pred[:, 1, i]))\nend\nplot_data = (; data_sequence, sequence, repeated_grid, label)\n\ndraw(\n    AoG.data(plot_data) *\n    mapping(\n        :repeated_grid => L\"x\",\n        :data_sequence => L\"u(x)\";\n        color=:label => \"\",\n        layout=:sequence => nonnumeric,\n        linestyle=:label => \"\",\n    ) *\n    visual(Lines; linewidth=4),\n    scales(; Color=(; palette=:tab10), LineStyle = (; palette = [:solid, :dash]));\n    figure=(;\n        size=(1024, 1024),\n        title=\"Using FNO to solve the Burgers equation\",\n        titlesize=25,\n    ),\n    axis=(; xlabelsize=25, ylabelsize=25),\n    legend=(; label=L\"u(x)\", position=:bottom, labelsize=20),\n)","category":"page"}]
}
