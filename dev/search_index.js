var documenterSearchIndex = {"docs":
[{"location":"models/nomad/#Nonlinear-Manifold-Decoders-for-Operator-Learning-(NOMADs)","page":"NOMAD","title":"Nonlinear Manifold Decoders for Operator Learning (NOMADs)","text":"","category":"section"},{"location":"models/nomad/","page":"NOMAD","title":"NOMAD","text":"NOMADs are similar to DeepONets in the aspect that they can learn when the input and output function spaces are defined on different domains. Their architecture is different and use nonlinearity to the latent codes to obtain the operator approximation. The architecture involves an approximator to encode the input function space, which is directly concatenated with the input function coordinates, and passed into a decoder net to give the output function at the given coordinate.","category":"page"},{"location":"models/nomad/","page":"NOMAD","title":"NOMAD","text":"beginalign*\nu(y) xrightarrowmathcalA   beta \n quad searrow\n quad quad mathcalG_theta u(y) = mathcalD(beta y) \n quad nearrow \ny\nendalign*","category":"page"},{"location":"models/nomad/#Usage","page":"NOMAD","title":"Usage","text":"","category":"section"},{"location":"models/nomad/","page":"NOMAD","title":"NOMAD","text":"Let's try to learn the anti-derivative operator for","category":"page"},{"location":"models/nomad/","page":"NOMAD","title":"NOMAD","text":"u(x) = sin(alpha x)","category":"page"},{"location":"models/nomad/","page":"NOMAD","title":"NOMAD","text":"That is, we want to learn","category":"page"},{"location":"models/nomad/","page":"NOMAD","title":"NOMAD","text":"mathcalG  u rightarrow v ","category":"page"},{"location":"models/nomad/","page":"NOMAD","title":"NOMAD","text":"such that","category":"page"},{"location":"models/nomad/","page":"NOMAD","title":"NOMAD","text":"v(x) = fracdudx quad forall  x in 0 2pi  alpha in 05 1","category":"page"},{"location":"models/nomad/#Copy-pastable-code","page":"NOMAD","title":"Copy-pastable code","text":"","category":"section"},{"location":"models/nomad/","page":"NOMAD","title":"NOMAD","text":"using NeuralOperators, Lux, Random, Optimisers, Zygote, CairoMakie\n\nrng = Random.default_rng()\n\neval_points = 1\ndata_size = 128\ndim_y = 1\nm = 32\n\nxrange = range(0, 2π; length=m) .|> Float32\nu_data = zeros(Float32, m, data_size)\nα = 0.5f0 .+ 0.5f0 .* rand(Float32, data_size)\n\ny_data = rand(Float32, 1, eval_points, data_size) .* 2π\nv_data = zeros(Float32, eval_points, data_size)\nfor i in 1:data_size\n    u_data[:, i] .= sin.(α[i] .* xrange)\n    v_data[:, i] .= -inv(α[i]) .* cos.(α[i] .* y_data[1, :, i])\nend\n\nnomad = NOMAD(Chain(Dense(m => 8, σ), Dense(8 => 8, σ), Dense(8 => 7)),\n    Chain(Dense(8 => 4, σ), Dense(4 => 1)))\n\nps, st = Lux.setup(rng, nomad)\ndata = [((u_data, y_data), v_data)]\n\nfunction train!(model, ps, st, data; epochs=10)\n    losses = []\n    tstate = Training.TrainState(model, ps, st, Adam(0.01f0))\n    for _ in 1:epochs, (x, y) in data\n        _, loss, _, tstate = Training.single_train_step!(AutoZygote(), MSELoss(), (x, y),\n            tstate)\n        push!(losses, loss)\n    end\n    return losses\nend\n\nlosses = train!(nomad, ps, st, data; epochs=1000)\n\nlines(losses)","category":"page"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Pre-Built-Architectures","page":"API Reference","title":"Pre-Built Architectures","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"NOMAD\nDeepONet\nFourierNeuralOperator","category":"page"},{"location":"api/#NeuralOperators.NOMAD","page":"API Reference","title":"NeuralOperators.NOMAD","text":"NOMAD(approximator, decoder, concatenate)\n\nConstructs a NOMAD from approximator and decoder architectures. Make sure the output from approximator combined with the coordinate dimension has compatible size for input to decoder\n\nArguments\n\napproximator: Lux network to be used as approximator net.\ndecoder: Lux network to be used as decoder net.\n\nKeyword Arguments\n\nconcatenate: function that defines the concatenation of output from approximator and the coordinate dimension, defaults to concatenation along first dimension after vectorizing the tensors\n\nReferences\n\n[1] Jacob H. Seidman and Georgios Kissas and Paris Perdikaris and George J. Pappas, \"NOMAD: Nonlinear Manifold Decoders for Operator Learning\", doi: https://arxiv.org/abs/2206.03551\n\nExample\n\njulia> approximator_net = Chain(Dense(8 => 32), Dense(32 => 32), Dense(32 => 16));\n\njulia> decoder_net = Chain(Dense(18 => 16), Dense(16 => 16), Dense(16 => 8));\n\njulia> nomad = NOMAD(approximator_net, decoder_net);\n\njulia> ps, st = Lux.setup(Xoshiro(), nomad);\n\njulia> u = rand(Float32, 8, 5);\n\njulia> y = rand(Float32, 2, 5);\n\njulia> size(first(nomad((u, y), ps, st)))\n(8, 5)\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralOperators.DeepONet","page":"API Reference","title":"NeuralOperators.DeepONet","text":"DeepONet(branch, trunk, additional)\n\nConstructs a DeepONet from a branch and trunk architectures. Make sure that both the nets output should have the same first dimension.\n\nArguments\n\nbranch: Lux network to be used as branch net.\ntrunk: Lux network to be used as trunk net.\n\nKeyword Arguments\n\nadditional: Lux network to pass the output of DeepONet, to include additional operations for embeddings, defaults to nothing\n\nReferences\n\n[1] Lu Lu, Pengzhan Jin, George Em Karniadakis, \"DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators\", doi: https://arxiv.org/abs/1910.03193\n\nExample\n\njulia> branch_net = Chain(Dense(64 => 32), Dense(32 => 32), Dense(32 => 16));\n\njulia> trunk_net = Chain(Dense(1 => 8), Dense(8 => 8), Dense(8 => 16));\n\njulia> deeponet = DeepONet(branch_net, trunk_net);\n\njulia> ps, st = Lux.setup(Xoshiro(), deeponet);\n\njulia> u = rand(Float32, 64, 5);\n\njulia> y = rand(Float32, 1, 10, 5);\n\njulia> size(first(deeponet((u, y), ps, st)))\n(10, 5)\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralOperators.FourierNeuralOperator","page":"API Reference","title":"NeuralOperators.FourierNeuralOperator","text":"FourierNeuralOperator(\n    σ=gelu; chs::Dims{C}=(2, 64, 64, 64, 64, 64, 128, 1), modes::Dims{M}=(16,),\n    permuted::Val{perm}=False, kwargs...) where {C, M, perm}\n\nFourier neural operator is a operator learning model that uses Fourier kernel to perform spectral convolutions. It is a promising way for surrogate methods, and can be regarded as a physics operator.\n\nThe model is comprised of a Dense layer to lift (d + 1)-dimensional vector field to n-dimensional vector field, and an integral kernel operator which consists of four Fourier kernels, and two Dense layers to project data back to the scalar field of interest space.\n\nArguments\n\nσ: Activation function for all layers in the model.\n\nKeyword Arguments\n\nchs: A Tuple or Vector of the 8 channel size.\nmodes: The modes to be preserved. A tuple of length d, where d is the dimension of data.\npermuted: Whether the dim is permuted. If permuted = Val(false), the layer accepts data in the order of (ch, x_1, ... , x_d , batch). Otherwise the order is (x_1, ... , x_d, ch, batch).\n\nExample\n\njulia> fno = FourierNeuralOperator(gelu; chs=(2, 64, 64, 128, 1), modes=(16,));\n\njulia> ps, st = Lux.setup(Xoshiro(), fno);\n\njulia> u = rand(Float32, 2, 1024, 5);\n\njulia> size(first(fno(u, ps, st)))\n(1, 1024, 5)\n\n\n\n\n\n","category":"type"},{"location":"api/#Building-blocks","page":"API Reference","title":"Building blocks","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"OperatorConv\nSpectralConv\nOperatorKernel\nSpectralKernel","category":"page"},{"location":"api/#NeuralOperators.OperatorConv","page":"API Reference","title":"NeuralOperators.OperatorConv","text":"OperatorConv(ch::Pair{<:Integer, <:Integer}, modes::Dims,\n    ::Type{<:AbstractTransform}; init_weight=glorot_uniform,\n    permuted=Val(false))\n\nArguments\n\nch: A Pair of input and output channel size ch_in => ch_out, e.g. 64 => 64.\nmodes: The modes to be preserved. A tuple of length d, where d is the dimension of data.\n::Type{TR}: The transform to operate the transformation.\n\nKeyword Arguments\n\ninit_weight: Initial function to initialize parameters.\npermuted: Whether the dim is permuted. If permuted = Val(false), the layer accepts data in the order of (ch, x_1, ... , x_d, batch). Otherwise the order is (x_1, ... , x_d, ch, batch).\n\nExample\n\njulia> OperatorConv(2 => 5, (16,), FourierTransform{ComplexF32});\n\njulia> OperatorConv(2 => 5, (16,), FourierTransform{ComplexF32}; permuted=Val(true));\n\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralOperators.SpectralConv","page":"API Reference","title":"NeuralOperators.SpectralConv","text":"SpectralConv(args...; kwargs...)\n\nConstruct a OperatorConv with FourierTransform{ComplexF32} as the transform. See OperatorConv for the individual arguments.\n\nExample\n\njulia> SpectralConv(2 => 5, (16,));\n\njulia> SpectralConv(2 => 5, (16,); permuted=Val(true));\n\n\n\n\n\n\n","category":"function"},{"location":"api/#NeuralOperators.OperatorKernel","page":"API Reference","title":"NeuralOperators.OperatorKernel","text":"OperatorKernel(ch::Pair{<:Integer, <:Integer}, modes::Dims, transform::Type{TR},\n    act::A=identity; permuted=Val(false), kwargs...) where {TR <: AbstractTransform, A}\n\nArguments\n\nch: A Pair of input and output channel size ch_in => ch_out, e.g. 64 => 64.\nmodes: The modes to be preserved. A tuple of length d, where d is the dimension of data.\n::Type{TR}: The transform to operate the transformation.\n\nKeyword Arguments\n\nσ: Activation function.\npermuted: Whether the dim is permuted. If permuted = Val(true), the layer accepts data in the order of (ch, x_1, ... , x_d , batch). Otherwise the order is (x_1, ... , x_d, ch, batch).\n\nAll the keyword arguments are passed to the OperatorConv constructor.\n\nExample\n\njulia> OperatorKernel(2 => 5, (16,), FourierTransform{ComplexF64});\n\njulia> OperatorKernel(2 => 5, (16,), FourierTransform{ComplexF64}; permuted=Val(true));\n\n\n\n\n\n\n","category":"type"},{"location":"api/#NeuralOperators.SpectralKernel","page":"API Reference","title":"NeuralOperators.SpectralKernel","text":"SpectralKernel(args...; kwargs...)\n\nConstruct a OperatorKernel with FourierTransform{ComplexF32} as the transform. See OperatorKernel for the individual arguments.\n\nExample\n\njulia> SpectralKernel(2 => 5, (16,));\n\njulia> SpectralKernel(2 => 5, (16,); permuted=Val(true));\n\n\n\n\n\n\n","category":"function"},{"location":"api/#Transform-API","page":"API Reference","title":"Transform API","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"NeuralOperators.AbstractTransform","category":"page"},{"location":"api/#NeuralOperators.AbstractTransform","page":"API Reference","title":"NeuralOperators.AbstractTransform","text":"AbstractTransform\n\nInterface\n\nBase.ndims(<:AbstractTransform): N dims of modes\ntransform(<:AbstractTransform, x::AbstractArray): Apply the transform to x\ntruncate_modes(<:AbstractTransform, x_transformed::AbstractArray): Truncate modes that contribute to the noise\ninverse(<:AbstractTransform, x_transformed::AbstractArray): Apply the inverse transform to x_transformed\n\n\n\n\n\n","category":"type"},{"location":"tutorials/burgers/#Burgers-Equation-using-DeepONet","page":"Burgers Equation","title":"Burgers Equation using DeepONet","text":"","category":"section"},{"location":"tutorials/burgers/#Data-Loading","page":"Burgers Equation","title":"Data Loading","text":"","category":"section"},{"location":"tutorials/burgers/","page":"Burgers Equation","title":"Burgers Equation","text":"using DataDeps, MAT, MLUtils\nusing PythonCall, CondaPkg # For `gdown`\nusing Printf\n\nconst gdown = pyimport(\"gdown\")\n\nregister(\n    DataDep(\n    \"Burgers\",\n    \"\"\"\n    Burgers' equation dataset from\n    [fourier_neural_operator](https://github.com/zongyi-li/fourier_neural_operator)\n\n    mapping between initial conditions to the solutions at the last point of time \\\n    evolution in some function space.\n\n    u(x,0) -> u(x, time_end):\n\n      * `a`: initial conditions u(x,0)\n      * `u`: solutions u(x,t_end)\n    \"\"\",\n    \"https://drive.google.com/uc?id=16a8od4vidbiNR3WtaBPCSZ0T3moxjhYe\",\n    \"9cbbe5070556c777b1ba3bacd49da5c36ea8ed138ba51b6ee76a24b971066ecd\";\n    fetch_method=(url, local_dir) -> begin\n        pyconvert(String, gdown.download(url, joinpath(local_dir, \"Burgers_R10.zip\")))\n    end,\n    post_fetch_method=unpack\n)\n)\n\nfilepath = joinpath(datadep\"Burgers\", \"burgers_data_R10.mat\")\n\nconst N = 2048\nconst Δsamples = 2^3\nconst grid_size = div(2^13, Δsamples)\nconst T = Float32\n\nfile = matopen(filepath)\nx_data = reshape(T.(collect(read(file, \"a\")[1:N, 1:Δsamples:end])), N, :, 1)\ny_data = reshape(T.(collect(read(file, \"u\")[1:N, 1:Δsamples:end])), N, :, 1)\nclose(file)\n\nx_data = permutedims(x_data, (2, 1, 3))\ngrid = reshape(T.(collect(range(0, 1; length=grid_size)')), :, grid_size, 1)","category":"page"},{"location":"tutorials/burgers/#Model","page":"Burgers Equation","title":"Model","text":"","category":"section"},{"location":"tutorials/burgers/","page":"Burgers Equation","title":"Burgers Equation","text":"using Lux, NeuralOperators, Optimisers, Zygote, Random\nusing LuxCUDA\n\nconst cdev = cpu_device()\nconst gdev = gpu_device()\n\ndeeponet = DeepONet(;\n    branch=(size(x_data, 1), ntuple(Returns(32), 5)...),\n    trunk=(size(grid, 1), ntuple(Returns(32), 5)...),\n    branch_activation=tanh,\n    trunk_activation=tanh\n)\nps, st = Lux.setup(Random.default_rng(), deeponet) |> gdev;","category":"page"},{"location":"tutorials/burgers/#Training","page":"Burgers Equation","title":"Training","text":"","category":"section"},{"location":"tutorials/burgers/","page":"Burgers Equation","title":"Burgers Equation","text":"x_data_dev = x_data |> gdev\ny_data_dev = y_data |> gdev\ngrid_dev = grid |> gdev\n\nfunction loss_function(model, ps, st, ((v, y), u))\n    û, stₙ = model((v, y), ps, st)\n    return MAELoss()(û, u), stₙ, (;)\nend\n\nfunction train_model!(model, ps, st, data; epochs=5000)\n    train_state = Training.TrainState(model, ps, st, Adam(0.0001f0))\n\n    for epoch in 1:epochs\n        _, loss, _, train_state = Training.single_train_step!(\n            AutoZygote(), loss_function, data, train_state)\n\n        if epoch % 25 == 1 || epoch == epochs\n            @printf(\"Epoch %d: loss = %.6e\\n\", epoch, loss)\n        end\n    end\n\n    return train_state.parameters, train_state.states\nend\n\nps_trained, st_trained = train_model!(\n    deeponet, ps, st, ((x_data_dev, grid_dev), y_data_dev))","category":"page"},{"location":"tutorials/burgers/#Plotting","page":"Burgers Equation","title":"Plotting","text":"","category":"section"},{"location":"tutorials/burgers/","page":"Burgers Equation","title":"Burgers Equation","text":"using CairoMakie\n\npred = first(deeponet((x_data_dev, grid_dev), ps_trained, st_trained)) |> cdev\n\nbegin\n    fig = Figure(; size=(1024, 1024))\n\n    axs = [Axis(fig[i, j]) for i in 1:4, j in 1:4]\n    for i in 1:4, j in 1:4\n        idx = i + (j - 1) * 4\n        ax = axs[i, j]\n        l1 = lines!(ax, vec(grid), pred[idx, :, 1])\n        l2 = lines!(ax, vec(grid), y_data[idx, :, 1])\n\n        i == 4 && (ax.xlabel = \"x\")\n        j == 1 && (ax.ylabel = \"u(x)\")\n\n        if i == 1 && j == 1\n            axislegend(ax, [l1, l2], [\"Predictions\", \"Ground Truth\"])\n        end\n    end\n    linkaxes!(axs...)\n\n    fig[0, :] = Label(fig, \"Burgers Equation using DeepONet\"; tellwidth=false, font=:bold)\n\n    fig\nend","category":"page"},{"location":"models/fno/#Fourier-Neural-Operators-(FNOs)","page":"FNO","title":"Fourier Neural Operators (FNOs)","text":"","category":"section"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"FNOs are a subclass of Neural Operators that learn the learn the kernel Kappa_theta, parameterized on theta between function spaces:","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"(Kappa_thetau)(x) = int_D kappa_theta(a(x) a(y) x y) dy  quad forall x in D","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"The kernel makes up a block v_t(x) which passes the information to the next block as:","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"v^(t+1)(x) = sigma((W^(t)v^(t) + Kappa^(t)v^(t))(x))","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"FNOs choose a specific kernel kappa(xy) = kappa(x-y), converting the kernel into a convolution operation, which can be efficiently computed in the fourier domain.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"beginalign*\n(Kappa_thetau)(x) \n= int_D kappa_theta(x - y) dy  quad forall x in D\n= mathcalF^-1(mathcalF(kappa_theta) mathcalF(u))(x) quad forall x in D\nendalign*","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"where mathcalF denotes the fourier transform. Usually, not all the modes in the frequency domain are used with the higher modes often being truncated.","category":"page"},{"location":"models/fno/#Usage","page":"FNO","title":"Usage","text":"","category":"section"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"Let's try to learn the anti-derivative operator for","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"u(x) = sin(alpha x)","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"That is, we want to learn","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"mathcalG  u rightarrow v ","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"such that","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"v(x) = fracdudx quad forall  x in 0 2pi  alpha in 05 1","category":"page"},{"location":"models/fno/#Copy-pastable-code","page":"FNO","title":"Copy-pastable code","text":"","category":"section"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"<details>\n\n<summary> Click here to see copy-pastable code for this example! </summary>\n","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"using NeuralOperators, Lux, Random, Optimisers, Zygote, CairoMakie\n\nrng = Random.default_rng()\n\ndata_size = 128\nm = 32\n\nxrange = range(0, 2π; length=m) .|> Float32;\nu_data = zeros(Float32, m, 1, data_size);\nα = 0.5f0 .+ 0.5f0 .* rand(Float32, data_size);\nv_data = zeros(Float32, m, 1, data_size);\n\nfor i in 1:data_size\n    u_data[:, 1, i] .= sin.(α[i] .* xrange)\n    v_data[:, 1, i] .= -inv(α[i]) .* cos.(α[i] .* xrange)\nend\n\nfno = FourierNeuralOperator(gelu; chs=(1, 64, 64, 128, 1), modes=(16,), permuted=Val(true))\n\nps, st = Lux.setup(rng, fno);\ndata = [(u_data, v_data)];\n\nfunction train!(model, ps, st, data; epochs=10)\n    losses = []\n    tstate = Training.TrainState(model, ps, st, Adam(0.01f0))\n    for _ in 1:epochs, (x, y) in data\n        _, loss, _, tstate = Training.single_train_step!(AutoZygote(), MSELoss(), (x, y),\n            tstate)\n        push!(losses, loss)\n    end\n    return losses\nend\n\nlosses = train!(fno, ps, st, data; epochs=100)\n\nlines(losses)","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"</details>","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"using NeuralOperators, Lux, Random, Optimisers, Zygote, CairoMakie","category":"page"},{"location":"models/fno/#Constructing-training-data","page":"FNO","title":"Constructing training data","text":"","category":"section"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"First, we construct our training data.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"rng = Random.default_rng()","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"data_size is the number of observations.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"data_size = 128","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"m is the length of a single observation, you can also interpret this as the size of the grid we're evaluating our function on.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"m = 32","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"We instantiate the domain that the function operates on as a range from 0 to 2π, whose length is the grid size.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"xrange = range(0, 2π; length=m) .|> Float32;\nnothing #hide","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"Each value in the array here, α, will be the multiplicative factor on the input to the sine function.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"α = 0.5f0 .+ 0.5f0 .* rand(Float32, data_size);\nnothing #hide","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"Now, we create our data arrays.  We are storing all of the training data in a single array, in order to batch process them more efficiently.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"u_data = zeros(Float32, m, 1, data_size);\nv_data = zeros(Float32, m, 1, data_size);\nnothing #hide","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"and fill the data arrays with values. Here, u_data is","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"for i in 1:data_size\n    u_data[:, 1, i] .= sin.(α[i] .* xrange)\n    v_data[:, 1, i] .= -inv(α[i]) .* cos.(α[i] .* xrange)\nend","category":"page"},{"location":"models/fno/#Creating-the-model","page":"FNO","title":"Creating the model","text":"","category":"section"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"Finally, we get to the model itself.  We instantiate a FourierNeuralOperator and provide it several parameters.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"The first argument is the \"activation function\" for each neuron.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"The keyword arguments are:","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"chs is a tuple, representing the layer sizes for each layer.\nmodes is a 1-tuple, where the number represents the number of Fourier modes that are preserved, and the size of the tuple represents the number of dimensions.\npermuted indicates that the order of the arguments is permuted such that each column of the array represents a single observation.  This is substantially faster than the usual row access pattern, since Julia stores arrays by concatenating columns. Val(true) is another way of expressing true, but in the type domain, so that the compiler can see the value and use the appropriate optimizations.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"fno = FourierNeuralOperator(\n    gelu;                    # activation function\n    chs=(1, 64, 64, 128, 1), # channel weights\n    modes=(16,),             # number of Fourier modes to retain\n    permuted=Val(true)       # structure of the data means that columns are observations\n)","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"Now, we set up the model.  This function returns two things, a set of parameters and a set of states.  Since the operator is \"stateless\", the states are empty and will remain so.  The parameters are the weights of the neural network, and we will be modifying them in the training loop.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"ps, st = Lux.setup(rng, fno);\nnothing #hide","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"We construct data as a vector of tuples (input, output).  These are pre-batched, but for example if we had a lot of training data, we could dynamically load it, or create multiple batches.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"data = [(u_data, v_data)];\nnothing #hide","category":"page"},{"location":"models/fno/#Training-the-model","page":"FNO","title":"Training the model","text":"","category":"section"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"Now, we create a function to train the model. An \"epoch\" is basically a run over all input data, and the more epochs we have, the better the neural network gets!","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"function train!(model, ps, st, data; epochs=10)\n    # The `losses` array is used only for visualization,\n    # you don't actually need it to train.\n    losses = []\n    # Initialize a training state and an optimizer (Adam, in this case).\n    tstate = Training.TrainState(model, ps, st, Adam(0.01f0))\n    # Loop over epochs, then loop over each batch of training data, and step into the training:\n    for _ in 1:epochs\n        for (x, y) in data\n            _, loss, _, tstate = Training.single_train_step!(\n                AutoZygote(), MSELoss(), (x, y),\n                tstate)\n            push!(losses, loss)\n        end\n    end\n    return losses\nend","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"Now we train our model!","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"losses = @time train!(fno, ps, st, data; epochs=500)","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"We can plot the losses - you can see that at some point, we hit diminishing returns.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"lines(losses; axis=(; yscale=log10, ylabel=\"Loss\", xlabel=\"Epoch\"))","category":"page"},{"location":"models/fno/#Applying-the-model","page":"FNO","title":"Applying the model","text":"","category":"section"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"Let's try to actually apply this model using some input data.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"input_data = u_data[:, 1, 1]","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"This is our input data.  It's currently one-dimensional, but our neural network expects input in batched form, so we simply reshape it (a no-cost operation) to a 3d array with singleton dimensions.","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"reshaped_input = reshape(input_data, length(input_data), 1, 1)","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"Now we can pass this to Lux.apply:","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"output_data, st = Lux.apply(fno, reshaped_input, ps, st)","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"and plot it:","category":"page"},{"location":"models/fno/","page":"FNO","title":"FNO","text":"f, a, p = lines(dropdims(reshaped_input; dims=(2, 3)); label=\"u\")\nlines!(a, dropdims(output_data; dims=(2, 3)); label=\"Predicted\")\nlines!(a, v_data[:, 1, 1]; label=\"Expected\")\naxislegend(a)\n# Compute the absolute error and plot that too,\n# on a separate axis.\nabsolute_error = v_data[:, 1, 1] .- dropdims(output_data; dims=(2, 3))\na2, p2 = lines(f[2, 1], absolute_error; axis=(; ylabel=\"Error\"))\nrowsize!(f.layout, 2, Aspect(1, 1 / 8))\nlinkxaxes!(a, a2)\nf","category":"page"},{"location":"#NeuralOperators","page":"NeuralOperators.jl","title":"NeuralOperators","text":"","category":"section"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"NeuralOperators.jl is a package written in Julia to provide the architectures for learning mapping between function spaces, and learning grid invariant solution of PDEs.","category":"page"},{"location":"#Installation","page":"NeuralOperators.jl","title":"Installation","text":"","category":"section"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"On Julia 1.10+, you can install NeuralOperators.jl by running","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"import Pkg\nPkg.add(\"NeuralOperators\")","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"Currently provided operator architectures are :","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"Fourier Neural Operators (FNOs)\nDeepONets\nNonlinear Manifold Decoders for Operator Learning (NOMADs)","category":"page"},{"location":"#Reproducibility","page":"NeuralOperators.jl","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"</details>","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"</details>","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"using Pkg # hide\nPkg.status(; mode=PKGMODE_MANIFEST) # hide","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"</details>","category":"page"},{"location":"","page":"NeuralOperators.jl","title":"NeuralOperators.jl","text":"using TOML\nusing Markdown\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink_manifest = \"https://github.com/SciML/\" *\n                name *\n                \".jl/tree/gh-pages/v\" *\n                version *\n                \"/assets/Manifest.toml\"\nlink_project = \"https://github.com/SciML/\" *\n               name *\n               \".jl/tree/gh-pages/v\" *\n               version *\n               \"/assets/Project.toml\"\nMarkdown.parse(\"\"\"You can also download the\n[manifest]($link_manifest)\nfile and the\n[project]($link_project)\nfile.\n\"\"\")","category":"page"},{"location":"models/deeponet/#DeepONets","page":"DeepONet","title":"DeepONets","text":"","category":"section"},{"location":"models/deeponet/","page":"DeepONet","title":"DeepONet","text":"DeepONets are another class of networks that learn the mapping between two function spaces by encoding the input function space and the location of the output space. The latent code of the input space is then projected on the location laten code to give the output. This allows the network to learn the mapping between two functions defined on different spaces.","category":"page"},{"location":"models/deeponet/","page":"DeepONet","title":"DeepONet","text":"beginalign*\nu(y) xrightarrowtextbranch   b \n quad searrow\nquad quad mathcalG_theta u(y) = sum_k b_k t_k \n  quad nearrow \ny   xrightarrowtexttrunk    t  \nendalign*","category":"page"},{"location":"models/deeponet/#Usage","page":"DeepONet","title":"Usage","text":"","category":"section"},{"location":"models/deeponet/","page":"DeepONet","title":"DeepONet","text":"Let's try to learn the anti-derivative operator for","category":"page"},{"location":"models/deeponet/","page":"DeepONet","title":"DeepONet","text":"u(x) = sin(alpha x)","category":"page"},{"location":"models/deeponet/","page":"DeepONet","title":"DeepONet","text":"That is, we want to learn","category":"page"},{"location":"models/deeponet/","page":"DeepONet","title":"DeepONet","text":"mathcalG  u rightarrow v ","category":"page"},{"location":"models/deeponet/","page":"DeepONet","title":"DeepONet","text":"such that","category":"page"},{"location":"models/deeponet/","page":"DeepONet","title":"DeepONet","text":"v(x) = fracdudx quad forall  x in 0 2pi  alpha in 05 1","category":"page"},{"location":"models/deeponet/#Copy-pastable-code","page":"DeepONet","title":"Copy-pastable code","text":"","category":"section"},{"location":"models/deeponet/","page":"DeepONet","title":"DeepONet","text":"using NeuralOperators, Lux, Random, Optimisers, Zygote, CairoMakie\n\nrng = Random.default_rng()\n\neval_points = 1\ndata_size = 64\ndim_y = 1\nm = 32\n\nxrange = range(0, 2π; length=m) .|> Float32\nu_data = zeros(Float32, m, data_size)\nα = 0.5f0 .+ 0.5f0 .* rand(Float32, data_size)\n\ny_data = rand(Float32, 1, eval_points, data_size) .* 2π\nv_data = zeros(Float32, eval_points, data_size)\nfor i in 1:data_size\n    u_data[:, i] .= sin.(α[i] .* xrange)\n    v_data[:, i] .= -inv(α[i]) .* cos.(α[i] .* y_data[1, :, i])\nend\n\ndeeponet = DeepONet(\n    Chain(Dense(m => 8, σ), Dense(8 => 8, σ), Dense(8 => 8, σ)),\n    Chain(Dense(1 => 4, σ), Dense(4 => 8, σ))\n)\n\nps, st = Lux.setup(rng, deeponet)\ndata = [((u_data, y_data), v_data)]\n\nfunction train!(model, ps, st, data; epochs=10)\n    losses = []\n    tstate = Training.TrainState(model, ps, st, Adam(0.001f0))\n    for _ in 1:epochs, (x, y) in data\n        _, loss, _, tstate = Training.single_train_step!(AutoZygote(), MSELoss(), (x, y),\n            tstate)\n        push!(losses, loss)\n    end\n    return losses\nend\n\nlosses = train!(deeponet, ps, st, data; epochs=1000)\n\nlines(losses)","category":"page"}]
}
